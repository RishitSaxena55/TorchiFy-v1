# MyTorch-v1 ðŸš€

[![GitHub stars](https://img.shields.io/github/stars/RishitSaxena55/MyTorch-v1?style=social)](https://github.com/RishitSaxena55/MyTorch-v1)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A **PyTorch-compatible** deep learning library featuring:
- Custom Transformer architectures for **Automatic Speech Recognition (ASR)** and **Language Modeling**
- From-scratch implementations of core neural network components
- End-to-end training pipelines with advanced techniques like beam search and gradient accumulation

## ðŸ“¦ Key Features

### ðŸŽ™ï¸ Automatic Speech Recognition (ASR)
- **Encoder-Decoder Transformer** architecture with:
  - Multi-head self/cross-attention
  - Positional encoding
  - CTC head for alignment
- Support for:
  - Greedy/beam search decoding
  - SpecAugment data augmentation
  - Character/subword tokenization

### ðŸ“– Language Modeling
- **Decoder-only Transformer** (GPT-style) with:
  - Causal masking
  - Autoregressive generation
- Tokenization strategies:
  - Character-level
  - Byte Pair Encoding (BPE)
- Sampling methods:
  - Greedy, top-k, nucleus (top-p)

### âš™ï¸ Core Components
| Module | Implementations |
|--------|----------------|
| **Attention** | Scaled dot-product, multi-head |
| **Layers** | Linear, LayerNorm, FeedForward |
| **Optimization** | AdamW, LR scheduling |
| **Regularization** | Dropout, label smoothing |

## ðŸ“Š Architecture Diagrams

### Encoder-Decoder Transformer (ASR)
```mermaid
graph TD
    A[80-dim FBANK Features] --> B[SpeechEmbedding]
    B --> C[+PositionalEncoding]
    C --> D[Encoder Layers]
    D --> E[Decoder Layers]
    E --> F[Linear + Softmax]
    F --> G[Character/Subword Predictions]

graph LR
    A[Token Embeddings] --> B[+PositionalEncoding]
    B --> C[Decoder Layers]
    C --> D[Linear + Softmax]
    D --> E[Next-Token Predictions]

git clone https://github.com/RishitSaxena55/MyTorch-v1.git
cd MyTorch-v1
pip install -r requirements.txt

MyTorch-v1/
â”œâ”€â”€ hw4lib/                 # Main library
â”‚   â”œâ”€â”€ data/               # Dataset handling
â”‚   â”‚   â”œâ”€â”€ asr_dataset.py  # Speech dataset loader
â”‚   â”‚   â”œâ”€â”€ lm_dataset.py   # Text dataset loader
â”‚   â”‚   â””â”€â”€ tokenizer.py    # Tokenization strategies
â”‚   â”œâ”€â”€ model/              # Transformer components
â”‚   â”‚   â”œâ”€â”€ sublayers.py    # Attention/FFN layers
â”‚   â”‚   â”œâ”€â”€ transformers.py # Full architectures
â”‚   â”‚   â””â”€â”€ positional_encoding.py
â”‚   â”œâ”€â”€ decoding/           # Generation algorithms
â”‚   â””â”€â”€ trainers/           # Training pipelines
â”œâ”€â”€ mytorch/                # Custom NN components
â”‚   â””â”€â”€ nn/
â”‚       â”œâ”€â”€ linear.py       # Fully-connected layer
â”‚       â”œâ”€â”€ activation.py   # Softmax/GELU
â”‚       â””â”€â”€ attention/      # Attention mechanisms
â””â”€â”€ tests/                  # Unit tests
