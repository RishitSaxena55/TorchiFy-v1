# âš¡ï¸TorchiFy-v1

![License](https://img.shields.io/github/license/RishitSaxena55/TorchiFy-v1)
![Tests](https://img.shields.io/badge/tests-passing-brightgreen)
![Language](https://img.shields.io/badge/python-3.10+-blue)
![Visitors](https://komarev.com/ghpvc/?username=RishitSaxena55&color=blue)
![Stars](https://img.shields.io/github/stars/RishitSaxena55/TorchiFy-v1?style=social)

> **"TorchiFy: Engineering Intelligence from First Principles â€” Build LLMs, Whisper-style ASR, and Transformer Architectures from Scratch."**

---

## ðŸš€ Introduction

**TorchiFy-v1** is a visionary, fully modular deep learning research framework written in pure PyTorch. It is designed to **build decoder-only, encoder-decoder, and speech-transformer architectures** entirely from scratchâ€”without relying on abstracted libraries. Whether you're building GPT-style chatbots, Whisper-like ASR systems, or next-gen AI interfaces, **TorchiFy** puts the architecture in your hands.

GitHub: [RishitSaxena55/TorchiFy-v1](https://github.com/RishitSaxena55/TorchiFy-v1)

---

## ðŸŒŒ Philosophy

At the core of **TorchiFy** lies a mission:  
> **Empower the curious to understand and build the next frontier of deep learning modelsâ€”line-by-line, token-by-token.**

TorchiFy is **educational**, **research-ready**, and **production-focused**, all at once.

---

## ðŸ§  What Makes TorchiFy Unique

| Feature                        | TorchiFy              | HuggingFace     | Fairseq        |
| ----------------------------- | --------------------- | --------------- | -------------- |
| Transformer from Scratch      | âœ… Yes (modular)       | âŒ No            | âŒ No           |
| PyTorch Only (No Frameworks)  | âœ… Zero abstraction    | âŒ Heavy deps    | âŒ              |
| ASR + Text Integration        | âœ… Native (Whisper)    | âš ï¸ Text-focused  | âœ…              |
| Fully Unit Tested             | âœ… High Coverage       | âš ï¸ Incomplete    | âœ…              |
| Educational Purpose           | âœ… Clear & Commented   | âŒ Obscure APIs  | âŒ              |
| Inference Optimization Tools  | âœ… Beam + Caching      | âš ï¸ Partial       | âš ï¸ Partial      |
| Training Engine               | âœ… Built-in            | âš ï¸ Requires Trainer | âš ï¸              |

---

## ðŸ§­ Project Overview

TorchiFy breaks the complexity of transformers into understandable, testable components:

- âœ… **Custom Multi-Head Attention**
- âœ… **Causal & Bidirectional Masks**
- âœ… **Sinusoidal, Learned & Rotary Embeddings**
- âœ… **Flexible Transformer Blocks (Decoder, Encoder, Hybrid)**
- âœ… **Audio + Text Frontends**
- âœ… **Modular Beam Search & Greedy Decoding**
- âœ… **End-to-End Whisper-like ASR**
- âœ… **LLM Pretraining Routines**
- âœ… **FP16 Support & Caching for Deployment**

---

## ðŸ’¥ Use TorchiFy For

- ðŸ’¬ **LLM Prototyping:** GPT-style transformer decoder with full training pipeline.
- ðŸ§  **ASR Systems:** Whisper-style encoder-decoder for speech-to-text.
- ðŸŽ“ **Teaching & Research:** Transformer mechanics, custom experiments.
- âš—ï¸ **Inference Research:** Modify attention sparsity, decoding speed, etc.
- ðŸ“¦ **Production Deployment:** Modular hooks for optimization, quantization.

---

## ðŸ§¬ Core Modules

| Folder             | Purpose                                                                 |
| ------------------| ------------------------------------------------------------------------ |
| `torchiFy/core/`   | Core transformer blocks (attention, embeddings, feedforward, etc.)     |
| `torchiFy/models/` | Model definitions: GPT, Whisper, Encoder-Decoder Transformers          |
| `torchiFy/nn/`     | Custom attention, masking, rotary/positional embeddings                |
| `torchiFy/audio/`  | Speech preprocessing: MFCC, Log-Mel, audio tokenizers                  |
| `torchiFy/train/`  | Training loop, loss functions, optimizers, schedulers                  |
| `torchiFy/decode/` | Greedy decoding, beam search, LM scoring                               |
| `torchiFy/utils/`  | Logging, timing, caching, weight loading/saving                        |
| `torchiFy/tests/`  | Full unit and integration test suite                                   |
| `configs/`         | YAML config files for models and training                              |
| `scripts/`         | Pretraining, inference, dataset setup scripts                          |

---

## ðŸ§± File Structure Explained

```
TorchiFy-v1/
â”œâ”€â”€ torchiFy/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ attention.py
â”‚   â”‚   â”œâ”€â”€ feedforward.py
â”‚   â”‚   â”œâ”€â”€ embeddings.py
â”‚   â”‚   â”œâ”€â”€ transformer_block.py
â”‚   â”œâ”€â”€ nn/
â”‚   â”‚   â”œâ”€â”€ utils.py
â”‚   â”‚   â”œâ”€â”€ mask.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ gpt.py
â”‚   â”‚   â”œâ”€â”€ whisper.py
â”‚   â”‚   â”œâ”€â”€ encoder_decoder.py
â”‚   â”œâ”€â”€ decode/
â”‚   â”‚   â”œâ”€â”€ beam.py
â”‚   â”‚   â”œâ”€â”€ greedy.py
â”‚   â”œâ”€â”€ audio/
â”‚   â”‚   â”œâ”€â”€ preprocess.py
â”‚   â”‚   â”œâ”€â”€ tokenizer.py
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ engine.py
â”‚   â”‚   â”œâ”€â”€ losses.py
â”‚   â”‚   â”œâ”€â”€ optim.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ logger.py
â”‚   â”‚   â”œâ”€â”€ timer.py
â”‚   â”‚   â”œâ”€â”€ weights.py
â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â”œâ”€â”€ test_attention.py
â”‚   â”‚   â”œâ”€â”€ test_gpt.py
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ gpt_small.yaml
â”‚   â”œâ”€â”€ whisper_base.yaml
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_gpt.py
â”‚   â”œâ”€â”€ infer_speech.py
â”‚   â”œâ”€â”€ convert_audio.py
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## ðŸ”§ Installation

```bash
git clone https://github.com/RishitSaxena55/TorchiFy-v1.git
cd TorchiFy-v1
pip install -r requirements.txt
```

---

## ðŸš¦ Getting Started

### ðŸ§ª Run a Unit Test
```bash
pytest torchiFy/tests/
```

### ðŸ”¥ Train a GPT-like Model
```bash
python scripts/train_gpt.py --config configs/gpt_small.yaml
```

### ðŸŽ™ï¸ Infer from Speech Input
```bash
python scripts/infer_speech.py --audio sample.wav --config configs/whisper_base.yaml
```

---

## ðŸ§  Key Innovations

- **No Hidden Blackboxes**
- **Custom Masking Engines**
- **Rotary Positional Embeddings**
- **Audio-Aware Transformer**
- **Beam Search from Scratch**
- **Doc-rich Modules**

---

## ðŸ§ª Testing & Validation

```bash
pytest torchiFy/tests/
```

---

## ðŸ› ï¸ Inference Strategies

- âœ… Greedy decoding
- âœ… Beam decoding with penalties
- âœ… CTC + LM Rescoring
- âœ… Caching of key-value pairs
- âœ… TorchScript-compatible exports

---

## ðŸŒ Future Roadmap

- [ ] FlashAttention Integration
- [ ] Quantized Attention Blocks
- [ ] Mixture of Experts
- [ ] Streaming ASR
- [ ] RLHF + PPO Support
- [ ] Vision Transformers

---

## ðŸ§‘â€ðŸ’» Contributing

We welcome contributors:

- ðŸ” Add new models or ASR frontends
- ðŸ“Š Improve test coverage
- ðŸ§  Propose architectural variants
- ðŸ“š Help write tutorials

---

## ðŸ“œ License

This project is licensed under the **MIT License**.

---

## âœ¨ Final Note

> **"TorchiFy is not just codeâ€”it's a canvas for building the next frontier of machine intelligence. Let's build from scratch, understand deeply, and create openly."**

**GitHub**: [RishitSaxena55/TorchiFy-v1](https://github.com/RishitSaxena55/TorchiFy-v1)  
**Author**: [Rishit Saxena](https://www.linkedin.com/in/rishit-saxena-12922531b/)
